{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_examples/first500gen.txt','r') as f:\n",
    "    docs = f.read().split('\\n<NEXTDOCUMENT>\\n')\n",
    "distinct_chars = list(set(''.join(docs)))\n",
    "n_chars = len(distinct_chars)\n",
    "char_dict = {c:idx for idx,c in enumerate(distinct_chars)}\n",
    "one_hot_docs = [nn.functional.one_hot(torch.tensor([char_dict[c] for c in doc]),num_classes=n_chars).float() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class character_embedder(nn.Module):\n",
    "    def __init__(self,n_chars):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(n_chars,16)\n",
    "        self.dense = nn.Linear(16,n_chars)\n",
    "        self.act = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        return self.sig(self.dense(x))\n",
    "    \n",
    "\n",
    "class character_embedder_sftmx(nn.Module):\n",
    "    def __init__(self,n_chars):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(n_chars,16)\n",
    "        self.dense = nn.Linear(16,n_chars)\n",
    "        self.sft = nn.Softmax(1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.sin(self.hidden(x))\n",
    "        return self.sft(self.dense(x))\n",
    "    \n",
    "\n",
    "class character_embedder_any_n_next(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(64,2)\n",
    "        self.dense = nn.Linear(2,2)\n",
    "        self.sft = nn.Softmax(1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.sin(self.hidden(x))\n",
    "        return self.sft(self.dense(x))\n",
    "\n",
    "\n",
    "def create_n_next_samples(doc,next_model,prev_model,char_dict,n=5):\n",
    "    next_features = next_model.hidden.weight.T[[char_dict[c] for c in doc[:-n]]]\n",
    "    prev_features = prev_model.hidden.weight.T[[char_dict[c] for c in doc[:-n]]]\n",
    "    samples = torch.vstack([torch.hstack([next_features,prev_features,\n",
    "                                          next_model.hidden.weight.T[[char_dict[c] for _ in doc[:-n]]],\n",
    "                                          prev_model.hidden.weight.T[[char_dict[c] for _ in doc[:-n]]]])\n",
    "                                          for c in char_dict.keys()])\n",
    "    labels = torch.tensor([1 if c_target in doc[idx:idx+n] else 0 for idx in range(len(doc)-n) for c_target in char_dict.keys()])\n",
    "    return samples,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc1_ch_feats,doc1_ch_labels = create_n_next_samples(docs[0],character_embedder_model_sft,character_embedder_model_sft,char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_embedder_model_sft = character_embedder_sftmx(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective2 = nn.BCELoss()\n",
    "optimizer2 = torch.optim.SGD(character_embedder_model_sft.parameters(),.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_counters_dict = {c:Counter(reduce(list.__add__,[[d[i+1] for i in range(len(d)-1) if d[i]==c] for d in docs])) for c in char_dict.keys()}\n",
    "next_totals = [sum(next_counters_dict[c].values()) for c in char_dict.keys()]\n",
    "next_expecteds = torch.tensor([[next_counters_dict[c][c2]/t for c2 in char_dict.keys()]\n",
    "             for c,t in zip(char_dict.keys(),next_totals)]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998 0.022148 0.022148           \r"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "average_loss = 0\n",
    "reporting_cadence = 10\n",
    "print_cadence = 3\n",
    "best_average = torch.tensor([torch.inf])\n",
    "c = 0\n",
    "break_criteria = 10\n",
    "bad_in_a_row = 0\n",
    "char_tensors = torch.eye(len(distinct_chars))\n",
    "for epoch in range(epochs):\n",
    "    preds = character_embedder_model_sft(char_tensors)\n",
    "    loss = objective2(preds,next_expecteds)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    optimizer2.zero_grad()\n",
    "    average_loss += loss\n",
    "    c += 1\n",
    "    if c>=print_cadence:\n",
    "        print(epoch,round(average_loss.detach().item()/print_cadence,6),round(best_average.detach().item()/print_cadence,6),' '*10,end='\\r')\n",
    "        if average_loss<best_average:\n",
    "            best_average = average_loss\n",
    "        c = 0\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7621e-02, 4.4432e-04, 9.4717e-01],\n",
       "        [5.1258e-05, 3.3872e-04, 9.9832e-01],\n",
       "        [1.9248e-04, 2.0082e-02, 1.3834e-02]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embedder_model_sft(nn.functional.one_hot(torch.tensor([char_dict['Q'],char_dict['q'],char_dict[' ']]),num_classes=n_chars).float())[:,[89,51,23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_prev_embedder_model_sft = character_embedder_sftmx(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(character_prev_embedder_model_sft.parameters(),.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_counters_dict = {c:Counter(reduce(list.__add__,[[d[i-1] for i in range(1,len(d)) if d[i]==c] for d in docs])) for c in char_dict.keys()}\n",
    "totals = [sum(prev_counters_dict[c].values()) for c in char_dict.keys()]\n",
    "expecteds = torch.tensor([[prev_counters_dict[c][c2]/t for c2 in char_dict.keys()]\n",
    "             for c,t in zip(char_dict.keys(),totals)]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998 0.020371 0.020371           \r"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "average_loss = 0\n",
    "reporting_cadence = 10\n",
    "print_cadence = 3\n",
    "best_average = torch.tensor([torch.inf])\n",
    "c = 0\n",
    "break_criteria = 10\n",
    "bad_in_a_row = 0\n",
    "char_tensors = torch.eye(len(distinct_chars))\n",
    "for epoch in range(epochs):\n",
    "    preds = character_prev_embedder_model_sft(char_tensors)\n",
    "    loss = objective(preds,expecteds)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    average_loss += loss\n",
    "    c += 1\n",
    "    if c>=print_cadence:\n",
    "        print(epoch,round(average_loss.detach().item()/print_cadence,6),round(best_average.detach().item()/print_cadence,6),' '*10,end='\\r')\n",
    "        if average_loss<best_average:\n",
    "            best_average = average_loss\n",
    "        c = 0\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0015, 0.1215, 0.1648]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_prev_embedder_model_sft(nn.functional.one_hot(torch.tensor([char_dict['?']]),num_classes=n_chars).float())[:,[89,51,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_reps = torch.hstack((torch.sin(character_embedder_model_sft.hidden(char_tensors)),torch.sin(character_prev_embedder_model_sft.hidden(char_tensors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(char_reps,'vector_reps/Char_reps/char_reps_02072025.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector_reps/Char_reps/char_map.txt','w') as f:\n",
    "    f.write('<NEXT_CHAR>'.join(distinct_chars))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
