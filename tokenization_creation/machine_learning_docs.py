machine_learning_docs = [
    """Machine learning is an integral part of many commercial applications and researchprojects today, in areas ranging from medical diagnosis and treatment to finding your friends on social networks. Many people think that machine learning can only be applied by large companies with extensive research teams. In this book, we want to show you how easy it can be to build machine learning solutions yourself, and how to best go about it. With the knowledge in this book, you can build your own system for finding out how people feel on Twitter, or making predictions about global warming. The applications of machine learning are endless and, with the amount of data available today, mostly limited by your imagination.""",
    """This book is for current and aspiring machine learning practitioners looking to implement solutions to real-world machine learning problems. This is an introductory book requiring no previous knowledge of machine learning or artificial intelligence (AI). We focus on using Python and the scikit-learn library, and work through all the steps to create a successful machine learning application. The methods we introduce will be helpful for scientists and researchers, as well as data scientists working on commercial applications. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries.
We made a conscious effort not to focus too much on the math, but rather on the practical aspects of using machine learning algorithms. As mathematics (probability theory, in particular) is the foundation upon which machine learning is built, we won't go into the analysis of the algorithms in great detail. If you are interested in the mathematics of machine learning algorithms, we recommend the book e Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, which is available for free at the authors' website. We will also not describe how to write machine learning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries."""
    """There are many books on machine learning and AI. However, all of them are meant for graduate students or PhD students in computer science, and they're full of advanced mathematics. This is in stark contrast with how machine learning is being used, as a commodity tool in research and commercial applications. Today, applying machine learning does not require a PhD. However, there are few resources out there that fully cover all the important aspects of implementing machine learning in practice, without requiring you to take advanced math courses. We hope this book will help people who want to apply machine learning without reading up on years' worth of calculus, linear algebra, and probability theory.""",
    """Without the help and support of a large group of people, this book would never have existed.
I would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in particular Dawn Schanafelt, for helping Sarah and me make this book a reality. I want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt, and John Myles White, who took the time to read the early versions of this book and provided me with invaluable feedback—in addition to being some of the cornerstones of the scientific open source ecosystem.
I am forever thankful for the welcoming open source scientific Python community, especially the contributors to scikit-learn. Without the support and help from this community, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I would never have become a core contributor to scikit-learn or learned to understand this package as well as I do now. My thanks also go out to all the other contributors who donate their time to improve and maintain this package.
I'm also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning and gave me ideas for structuring a textbook. Among the people I talk to about machine learning, I specifically want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone.
My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways.
On the personal side, I want to thank my parents, Harald and Margot, and my sister, Miriam, for their continuing support and encouragement. I also want to thank the many people in my life whose love and friendship gave me the energy and support to undertake such a challenging task."""
    """For both supervised and unsupervised learning tasks, it is important to have a representation of your input data that a computer can understand. Often it is helpful to think of your data as a table. Each data point that you want to reason about (each email, each customer, each transaction) is a row, and each property that describes that data point (say, the age of a customer or the amount or location of a transaction) is a column. You might describe users by their age, their gender, when they created an account, and how often they have bought from your online shop. You might describe the image of a tumor by the grayscale values of each pixel, or maybe by using the size, shape, and color of the tumor.
Each entity or row here is known as a sample (or data point) in machine learning, while the columns—the properties that describe these entities—are called features. Later in this book we will go into more detail on the topic of building a good representation of your data, which is called feature extraction or feature engineering. You should keep in mind, however, that no machine learning algorithm will be able to make a prediction on data for which it has no information. For example, if the only feature that you have for a patient is their last name, no algorithm will be able to predict their gender. This information is simply not contained in your data. If you add another feature that contains the patient's first name, you will have much better luck, as it is often possible to tell the gender by a person's first name.""",
    """NumPy is one of the fundamental packages for scientific computing in Python. It contains functionality for multidimensional arrays, high-level mathematical func tions such as linear algebra operations and the Fourier transform, and pseudorandom number generators.
In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn takes in data in the form of NumPy arrays. Any data you're using will have to be con verted to a NumPy array. The core functionality of NumPy is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. A NumPy array looks like this:""",
    """We want to build a machine learning model from this data that can predict the spe cies of iris for a new set of measurements. But before we can apply our model to new measurements, we need to know whether it actually works—that is, whether we should trust its predictions.
Unfortunately, we cannot use the data we used to build the model to evaluate it. This is because our model can always simply remember the whole training set, and will therefore always predict the correct label for any point in the training set. This “remembering” does not indicate to us whether our model will generalize well (in other words, whether it will also perform well on new data).
To assess the model's performance, we show it new data (data that it hasn't seen before) for which we have labels. This is usually done by splitting the labeled data we have collected (here, our 150 flower measurements) into two parts. One part of the data is used to build our machine learning model, and is called the training data or training set. The rest of the data will be used to assess how well the model works; this is called the test data, test set, or hold-out set.
scikit-learn contains a function that shuffles the dataset and splits it for you: the train_test_split function. This function extracts 75% of the rows in the data as the training set, together with the corresponding labels for this data. The remaining 25% of the data, together with the remaining labels, is declared as the test set. Deciding how much data you want to put into the training and the test set respectively is some what arbitrary, but using a test set containing 25% of the data is a good rule of thumb.
In scikit-learn, data is usually denoted with a capital X, while labels are denoted by a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics, where x is the input to a function and y is the output. Following more conventions from mathematics, we use a capital X because the data is a two-dimensional array (a matrix) and a lowercase y because the target is a one-dimensional array (a vector).
Let's call train_test_split on our data and assign the outputs using this nomencla ture:""",
    """Before building a machine learning model it is often a good idea to inspect the data, to see if the task is easily solvable without machine learning, or if the desired infor mation might not be contained in the data.
Additionally, inspecting your data is a good way to find abnormalities and peculiari ties. Maybe some of your irises were measured using inches and not centimeters, for example. In the real world, inconsistencies in the data and unexpected measurements are very common.
One of the best ways to inspect data is to visualize it. One way to do this is by using a scatter plot. A scatter plot of the data puts one feature along the x-axis and another along the y-axis, and draws a dot for each data point. Unfortunately, computer screens have only two dimensions, which allows us to plot only two (or maybe three) features at a time. It is difficult to plot datasets with more than three features this way. One way around this problem is to do a pair plot, which looks at all possible pairs of features. If you have a small number of features, such as the four we have here, this is quite reasonable. You should keep in mind, however, that a pair plot does not show the interaction of all of features at once, so some interesting aspects of the data may not be revealed when visualizing it this way.
Figure 1-3 is a pair plot of the features in the training set. The data points are colored according to the species the iris belongs to. To create the plot, we first convert the NumPy array into a pandas DataFrame. pandas has a function to create pair plots called scatter_matrix. The diagonal of this matrix is filled with histograms of each feature:""",
    """Now we can start building the actual machine learning model. There are many classi fication algorithms in scikit-learn that we could use. Here we will use a k-nearest neighbors classifier, which is easy to understand. Building this model only consists of storing the training set. To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point. Then it assigns the label of this training point to the new data point.
The k in k-nearest neighbors signifies that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the train ing (for example, the closest three or five neighbors). Then, we can make a prediction using the majority class among these neighbors. We will go into more detail about this in Chapter 2; for now, we'll use only a single neighbor.
All machine learning models in scikit-learn are implemented in their own classes, which are called Estimator classes. The k-nearest neighbors classification algorithm is implemented in the KNeighborsClassifier class in the neighbors module. Before we can use the model, we need to instantiate the class into an object. This is when we will set any parameters of the model. The most important parameter of KNeighbor sClassifier is the number of neighbors, which we will set to 1:""",
    """Let's summarize what we learned in this chapter. We started with a brief introduction to machine learning and its applications, then discussed the distinction between supervised and unsupervised learning and gave an overview of the tools we'll be using in this book. Then, we formulated the task of predicting which species of iris a particular flower belongs to by using physical measurements of the flower. We used a dataset of measurements that was annotated by an expert with the correct species to build our model, making this a supervised learning task. There were three possible species, setosa, versicolor, or virginica, which made the task a three-class classification problem. The possible species are called classes in the classification problem, and the species of a single iris is called its label.
The Iris dataset consists of two NumPy arrays: one containing the data, which is referred to as X in scikit-learn, and one containing the correct or desired outputs, which is called y. The array X is a two-dimensional array of features, with one row per data point and one column per feature. The array y is a one-dimensional array, which here contains one class label, an integer ranging from 0 to 2, for each of the samples.
We split our dataset into a training set, to build our model, and a test set, to evaluate how well our model will generalize to new, previously unseen data.
We chose the k-nearest neighbors classification algorithm, which makes predictions for a new data point by considering its closest neighbor(s) in the training set. This is implemented in the KNeighborsClassifier class, which contains the algorithm that builds the model as well as the algorithm that makes a prediction using the model. We instantiated the class, setting parameters. Then we built the model by calling the fit method, passing the training data (X_train) and training outputs (y_train) as parameters. We evaluated the model using the score method, which computes the accuracy of the model. We applied the score method to the test set data and the test set labels and found that our model is about 97% accurate, meaning it is correct 97% of the time on the test set
This gave us the confidence to apply the model to new data (in our example, new flower measurements) and trust that the model will be correct about 97% of the time.
Here is a summary of the code needed for the whole training and evaluation procedure:""",
    """As we mentioned earlier, supervised machine learning is one of the most commonly used and successful types of machine learning. In this chapter, we will describe super vised learning in more detail and explain several popular supervised learning algo rithms. We already saw an application of supervised machine learning in Chapter 1: classifying iris flowers into several species using physical measurements of the flowers.
Remember that supervised learning is used whenever we want to predict a certain outcome from a given input, and we have examples of input/output pairs. We build a machine learning model from these input/output pairs, which comprise our training set. Our goal is to make accurate predictions for new, never-before-seen data. Super vised learning often requires human effort to build the training set, but afterward automates and often speeds up an otherwise laborious or infeasible task.""",
    """The iris example, on the other hand, is an example of a multiclass classification prob lem. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages.
For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms). Predicting a person's annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an amount, and can be any number in a given range. Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an arbitrary number.
An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting annual income. There is a clear continuity in the output. Whether a person makes $40,000 or $40,001 a year does not make a tangible difference, even though these are different amounts of money; if our algorithm predicts $39,999 or $40,001 when it should have predicted $40,000, we don't mind that much.
By contrast, for the task of recognizing the language of a website (which is a classifi cation problem), there is no matter of degree. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French""",
    """Again, the prediction is shown as the color of the cross. You can see that the predic tion for the new data point at the top left is not the same as the prediction when we used only one neighbor.
While this illustration is for a binary classification problem, this method can be applied to datasets with any number of classes. For more classes, we count how many neighbors belong to each class and again predict the most common class.
Now let's look at how we can apply the k-nearest neighbors algorithm using scikitlearn. First, we split our data into a training and a test set so we can evaluate general ization performance, as discussed in Chapter 1""",
    """The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam.
Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a “teacher” provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem.""",
    """Python has become the lingua franca for many data science applications. It combines the power of general-purpose programming languages with the ease of use of domain-specific scripting languages like MATLAB or R. Python has libraries for data loading, visualization, statistics, natural language processing, image processing, and more. This vast toolbox provides data scientists with a large array of general- and special-purpose functionality. One of the main advantages of using Python is the abil ity to interact directly with the code, using a terminal or other tools like the Jupyter Notebook, which we'll look at shortly. Machine learning and data analysis are funda mentally iterative processes, in which the data drives the analysis. It is essential for these processes to have tools that allow quick iteration and easy interaction.
As a general-purpose programming language, Python also allows for the creation of complex graphical user interfaces (GUIs) and web services, and for integration into existing systems.""",
    """There are a variety of techniques and methods in machine learning. Some of the most important are:
Preprocessing: Before data can be analyzed, it often needs to be preprocessed. This can include removing noise or irrelevant data, filling in missing data or converting categorical data into numerical formats.
Algorithms: There are many different algorithms in machine learning, from simple linear regressions to complex neural networks. Choosing the right algorithm depends on the type of data and the type of prediction to be made.
Validation: After a model has been trained, it needs to be validated. This usually means testing the model with a separate data set to see how well it makes predictions.
Optimization: Finally, a model can often be optimized using various techniques. This may involve adjusting the parameters of the model or using techniques such as regularization or boosting.""",
    """Machine learning has a variety of applications in many different fields. Some examples are:
Medicine: machine learning is used to diagnose diseases, create treatment plans and even develop new medicine.
Finance: In the financial sector, machine learning is used to assess credit risk, predict stock prices and detect fraud.
Marketing: In marketing, machine learning is used to analyze customer data, create personalized advertising and predict buying behavior.
Transport: In the transport sector, machine learning is used to predict traffic patterns, optimize routes and improve autonomous driving technology.
Image and speech recognition: Machine learning enables computers to understand images and recognize speech, enabling applications such as facial recognition and voice assistants.""",
    """The main difference between machine learning (ML) and deep learning (DL) lies in their complexity and the techniques they use. Machine learning is a branch of artificial intelligence that enables computers to learn from data and make decisions without being explicitly programmed. It includes a variety of algorithms such as decision trees, random forests or support vector machines that are based on structured data.
Deep learning, on the other hand, is a specialized subcategory of machine learning that uses artificial neural networks to recognize highly complex patterns in large, unstructured data sets.
While machine learning is often used for simpler tasks, deep learning is particularly effective for tasks such as image and speech recognition, where deeper layers of neurons allow abstract concepts to be captured.""",
    """Despite the many advances in this field, there are also challenges and ethical issues that arise in connection with machine learning.
Data privacy: As ML models often require large amounts of data to be effective, data privacy issues can arise. It is important that companies handle data responsibly and respect the privacy of their users.
Bias in data: If the data used to train a model is biased, then the model will also be biased. This can lead to certain groups of people being treated unfairly.
Transparency: Many ML models are “black boxes”, meaning that it is difficult to understand why they make a particular prediction. This can make it difficult to explain or justify a model's decisions.
The Future of Machine Learning
The future of machine learning looks very promising. With advances in areas such as deep learning and reinforcement learning, ML models are becoming more powerful and versatile.
An exciting trend in this area is Automated Machine Learning (AutoML), where the process of training and optimizing ML models is automated. This could enable even more people to reap the benefits of machine learning without having to be experts in the field.
Furthermore, machine learning is expected to be increasingly used in various industries, from healthcare to finance to manufacturing. With the ability to recognize patterns in large amounts of data and make accurate predictions, machine learning has the potential to transform many aspects of our society.""",
    """Typical results from machine learning applications usually include web search results, real-time ads on web pages and mobile devices, email spam filtering, network intrusion detection, and pattern and image recognition. All these are the by-products of using machine learning to analyze massive volumes of data.
Traditionally, data analysis was trial and error-based, an approach that became increasingly impractical thanks to the rise of large, heterogeneous data sets. Machine learning provides smart alternatives for large-scale data analysis. Machine learning can produce accurate results and analysis by developing fast and efficient algorithms and data-driven models for real-time data processing.
Pro Tip: For more on Big Data and how it's revolutionizing industries globally, check out our “What is Big Data?” article.
According to Marketwatch, the global machine learning market is expected to grow at a healthy rate of over 45.9 percent during the period of 2017-2025. If this trend holds, then we will see a greater use of machine learning across a wide spectrum of industries worldwide. Machine learning is here to stay!""",
    """Since deep learning and machine learning tend to be used interchangeably, it's worth noting the nuances between the two. Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence. However, neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks.
The way in which deep learning and machine learning differ is in how each algorithm learns. "Deep" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn't necessarily require a labeled dataset. The deep learning process can ingest unstructured data in its raw form (e.g., text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of large amounts of data. You can think of deep learning as "scalable machine learning" as Lex Fridman notes in this MIT lecture1.
Classical, or "non-deep," machine learning is more dependent on human intervention to learn. Human experts determine the set of features to understand the differences between data inputs, usually requiring more structured data to learn.
Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. The “deep” in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the input and the output—can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network.
Deep learning and neural networks are credited with accelerating progress in areas such as computer vision, natural language processing, and speech recognition.""",
    """Depending on your budget, need for speed and precision required, each algorithm type—supervised, unsupervised, semi-supervised, or reinforcement—has its own advantages and disadvantages. For example, decision tree algorithms are used for both predicting numerical values (regression problems) and classifying data into categories. Decision trees use a branching sequence of linked decisions that may be represented with a tree diagram. A prime advantage of decision trees is that they are easier to validate and audit than a neural network. The bad news is that they can be more unstable than other decision predictors.
Overall, there are many advantages to machine learning that businesses can leverage for new efficiencies. These include machine learning identifying patterns and trends in massive volumes of data that humans might not spot at all. And this analysis requires little human intervention: just feed in the dataset of interest and let the machine learning system assemble and refine its own algorithms—which will continually improve with more data input over time. Customers and users can enjoy a more personalized experience as the model learns more with every experience with that person.
On the downside, machine learning requires large training datasets that are accurate and unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering sufficient data and having a system robust enough to run it might also be a drain on resources. Machine learning can also be prone to error, depending on the input. With too small a sample, the system could produce a perfectly logical algorithm that is completely wrong or misleading. To avoid wasting budget or displeasing customers, organizations should act on the answers only when there is high confidence in the output.""",
    """Assess follow-up recommendations in radiology reports, develop and assess traditional machine learning (TML) and deep learning (DL) models in identifying follow-up, and benchmark them against a natural language processing (NLP) system.
Methods:
This HIPAA-compliant, IRB approved study, was performed at an academic medical center generating >500,000 radiology reports annually. 1,000 randomly-selected ultrasound, x-ray, computed tomography and magnetic resonance imaging reports generated in 2016 were manually reviewed and annotated for follow-up recommendations. Traditional machine learning (Support Vector Machines, Random Forest, Logistic Regression) and deep learning (Recurrent Neural Nets) algorithms were constructed and trained on 850 reports (training data), with subsequent optimization of model architectures and parameters. Precision, recall and F1-score were calculated on the remaining 150 reports (test data). A previously-developed and validated NLP system (iSCOUT) was also applied to the test data, with equivalent metrics calculated.
Results:
12.7% of reports had follow-up recommendations. The TML algorithms achieved F1 scores of 0.75 (Random Forest), 0.83 (Logistic Regression), and 0.85 (Support Vector Machine) on the test data. DL Recurrent Neural Nets had an F1 score of 0.71; iSCOUT also had an F1 score of 0.71. Performance of both TML and DL methods by F1-scores appeared to plateau after 500–700 samples while training.
Conclusion:
TML and DL are feasible methods to identify follow-up recommendations. These methods have great potential for near real-time monitoring of follow up recommendations in radiology reports.""",
    """Longitudinal studies that use surveys to follow participants over time are indispensable research tools for studying changes in health and quality of life and for examining associations between exposures and subsequent outcomes1. However, the success of these studies hinges on sustained engagement from participants. Participant nonresponse, which refers to the phenomenon in which participants who initially enroll in a study and subsequently do not respond to one or more follow-up survey waves, may pose a risk to the validity of the research2. Associations between exposures and/or outcomes of interest and survey nonresponse may lead to spurious findings and potentially limit the generalizability to the larger source population. To mitigate any potential impact, studies aim to minimize nonresponse by using strategies such as maintaining regular contact with participants, providing incentives for survey completion, and minimizing the burden on participants3,4.
In recent years, predictive analytics has emerged as a promising tool5 to identify patterns in survey data and predict participant nonresponse, which may then be utilized to inform outreach, recruitment, and retention strategies in cohort studies6. Data-driven models also adapt and learn from new data. These models can be trained on a subset of the data and then be used to make predictions on new data as they become available, allowing researchers to continuously tune their models and make more accurate predictions over time.
The effectiveness of machine learning in predicting survey response remains a subject of ongoing debate within the literature. Kern et al.7,8 employed machine learning algorithms to predict nonresponse in two separate longitudinal studies, both sampled German civilian populations and employed web and paper surveys. Publications on both studies used a temporal prequential approach, either sliding or growing window as described by Cerqueira et al.9, and the second publication incorporated indicators of historical survey response, such as completion of the last survey and how many of the previous three waves were completed, as predictors. Although both studies achieved high prediction performance with predicted probabilities ≥ 80%, the likelihood of correctly predicting that a participant will not respond was relatively low, with a predicted probability ≤ 50%, indicating a bias in the model for predicting response.
Another method employed in research on survey response behavior is latent class analysis (LCA) used to classify patterns of historical response. LCA classifies individuals into meaningful but unmeasured groups by identifying underlying patterns across a set of indicator variables using a probability model10. Cernat and Sakshaug11 used LCA to classify participants enrolled in the Understanding Society Innovation Panel based on survey response over five waves and examined the relationship between the historical survey response with later survey response to Wave 10. In logistic regression models, the LCA variable was strongly associated with both response and response mode.
In contrast, predictive models have been criticized for their “black box” nature. Jankowsky and Schroeders12 examined prediction models among two additional longitudinal cohorts, one based in the U.S. and the other in Germany, and found that the more complex generalized boosted regression models did not substantially improve prediction performance when compared with logistic regression. The authors concluded that since machine learning models are more complex to interpret than more traditional models such as logistic regression, they may not be optimally suited for predicting survey nonresponse.
Considering the mixed results in the literature surrounding the effectiveness of predictive analytics, we sought to determine the utility of predictive analytics for survey response in the Millennium Cohort Study (hereafter the Study), the largest and longest running research effort of U.S. service members and veterans that aims to investigate the long-term physical, psychological, and behavioral health impacts of military service13. The first panel of participants (Panel 1) of the Study includes over 77,000 participants with diverse demographics, a larger sample size than that of the aforementioned studies which enrolled only 3,000 to 15,000 participants. Furthermore, our advantage lies in the rich historical data of the Study that enabled us to incorporate past survey responses as predictors in machine learning models. In this analysis, we used data from the Study to evaluate six supervised machine learning algorithms, both with and without LCA classification of historical response patterns and developed a high-skill classifier that estimated the likelihood of response to the 2021 follow-up survey among Panel 1 of the Study.""",
    """Machine learning is a branch of AI focused on building computer systems that learn from data. The breadth of ML techniques enables software applications to improve their performance over time.
ML algorithms are trained to find relationships and patterns in data. Using historical data as input, these algorithms can make predictions, classify information, cluster data points, reduce dimensionality and even generate new content. Examples of the latter, known as generative AI, include OpenAI's ChatGPT, Anthropic's Claude and GitHub Copilot.
Machine learning is widely applicable across many industries. For example, e-commerce, social media and news organizations use recommendation engines to suggest content based on a customer's past behavior. In self-driving cars, ML algorithms and computer vision play a critical role in safe road navigation. In healthcare, ML can aid in diagnosis and suggest treatment plans. Other common ML use cases include fraud detection, spam filtering, malware threat detection, predictive maintenance and business process automation.
While ML is a powerful tool for solving problems, improving business operations and automating tasks, it's also complex and resource-intensive, requiring deep expertise and significant data and infrastructure. Choosing the right algorithm for a task calls for a strong grasp of mathematics and statistics. Training ML algorithms often demands large amounts of high-quality data to produce accurate results. The results themselves, particularly those from complex algorithms such as deep neural networks, can be difficult to understand. And ML models can be costly to run and fine-tune.
Still, most organizations are embracing machine learning, either directly or through ML-infused products. According to a 2024 report from Rackspace Technology, AI spending in 2024 is expected to more than double compared with 2023, and 86% of companies surveyed reported seeing gains from AI adoption. Companies reported using the technology to enhance customer experience (53%), innovate in product design (49%) and support human resources (47%), among other applications.
TechTarget's guide to machine learning serves as a primer on this important field, explaining what machine learning is, how to implement it and its business applications. You'll find information on the various types of ML algorithms, challenges and best practices associated with developing and deploying ML models, and what the future holds for machine learning. Throughout the guide, there are hyperlinks to related articles that cover these topics in greater depth.""",
    """ML has played an increasingly important role in human society since its beginnings in the mid-20th century, when AI pioneers like Walter Pitts, Warren McCulloch, Alan Turing and John von Neumann laid the field's computational groundwork. Training machines to learn from data and improve over time has enabled organizations to automate routine tasks -- which, in theory, frees humans to pursue more creative and strategic work.
Machine learning has extensive and diverse practical applications. In finance, ML algorithms help banks detect fraudulent transactions by analyzing vast amounts of data in real time at a speed and accuracy humans cannot match. In healthcare, ML assists doctors in diagnosing diseases based on medical images and informs treatment plans with predictive models of patient outcomes. And in retail, many companies use ML to personalize shopping experiences, predict inventory needs and optimize supply chains.
ML also performs manual tasks that are beyond human ability to execute at scale -- for example, processing the huge quantities of data generated daily by digital devices. This ability to extract patterns and insights from vast data sets has become a competitive differentiator in fields like banking and scientific discovery. Many of today's leading companies, including Meta, Google and Uber, integrate ML into their operations to inform decision-making and improve efficiency.
Machine learning is necessary to make sense of the ever-growing volume of data generated by modern societies. The abundance of data humans create can also be used to further train and fine-tune ML models, accelerating advances in ML. This continuous learning loop underpins today's most advanced AI systems, with profound implications.
Philosophically, the prospect of machines processing vast amounts of data challenges humans' understanding of our intelligence and our role in interpreting and acting on complex information. Practically, it raises important ethical considerations about the decisions made by advanced ML models. Transparency and explainability in ML training and decision-making, as well as these models' effects on employment and societal structures, are areas for ongoing oversight and discussion.""",
    """Data science is considered a discipline, while data scientists are the practitioners within that field. Data scientists are not necessarily directly responsible for all the processes involved in the data science lifecycle. For example, data pipelines are typically handled by data engineers—but the data scientist may make recommendations about what sort of data is useful or required. While data scientists can build machine learning models, scaling these efforts at a larger level requires more software engineering skills to optimize a program to run more quickly. As a result, it's common for a data scientist to partner with machine learning engineers to scale machine learning models.
Data scientist responsibilities can commonly overlap with a data analyst, particularly with exploratory data analysis and data visualization. However, a data scientist's skillset is typically broader than the average data analyst. Comparatively speaking, data scientist leverage common programming languages, such as R and Python, to conduct more statistical inference and data visualization.
To perform these tasks, data scientists require computer science and pure science skills beyond those of a typical business analyst or data analyst. The data scientist must also understand the specifics of the business, such as automobile manufacturing, eCommerce, or healthcare.""",
    """It may be easy to confuse the terms “data science” and “business intelligence” (BI) because they both relate to an organization's data and analysis of that data, but they do differ in focus.
Business intelligence (BI) is typically an umbrella term for the technology that enables data preparation, data mining, data management, and data visualization. Business intelligence tools and processes allow end users to identify actionable information from raw data, facilitating data-driven decision-making within organizations across various industries. While data science tools overlap in much of this regard, business intelligence focuses more on data from the past, and the insights from BI tools are more descriptive in nature. It uses data to understand what happened before to inform a course of action. BI is geared toward static (unchanging) data that is usually structured. While data science uses descriptive data, it typically utilizes it to determine predictive variables, which are then used to categorize data or to make forecasts.
Data science and BI are not mutually exclusive—digitally savvy organizations use both to fully understand and extract value from their data.""",
    """Data scientists also gain proficiency in using big data processing platforms, such as Apache Spark, the open source framework Apache Hadoop, and NoSQL databases. They are also skilled with a wide range of data visualization tools, including simple graphics tools included with business presentation and spreadsheet applications (like Microsoft Excel), built-for-purpose commercial visualization tools like Tableau and IBM Cognos, and open source tools like D3.js (a JavaScript library for creating interactive data visualizations) and RAW Graphs. For building machine learning models, data scientists frequently turn to several frameworks like PyTorch, TensorFlow, MXNet, and Spark MLib.
Given the steep learning curve in data science, many companies are seeking to accelerate their return on investment for AI projects; they often struggle to hire the talent needed to realize data science project's full potential. To address this gap, they are turning to multipersona data science and machine learning (DSML) platforms, giving rise to the role of “citizen data scientist.”
Multipersona DSML platforms use automation, self-service portals, and low-code/no-code user interfaces so that people with little or no background in digital technology or expert data science can create business value using data science and machine learning. These platforms also support expert data scientists by also offering a more technical interface. Using a multipersona DSML platform encourages collaboration across the enterprise.""",
    """Cloud computing scales data science by providing access to additional processing power, storage, and other tools required for data science projects.
Since data science frequently leverages large data sets, tools that can scale with the size of the data is incredibly important, particularly for time-sensitive projects. Cloud storage solutions, such as data lakes, provide access to storage infrastructure, which are capable of ingesting and processing large volumes of data with ease. These storage systems provide flexibility to end users, allowing them to spin up large clusters as needed. They can also add incremental compute nodes to expedite data processing jobs, allowing the business to make short-term tradeoffs for a larger long-term outcome. Cloud platforms typically have different pricing models, such a per-use or subscriptions, to meet the needs of their end user—whether they are a large enterprise or a small startup.
Open source technologies are widely used in data science tool sets. When they're hosted in the cloud, teams don't need to install, configure, maintain, or update them locally. Several cloud providers, including IBM Cloud®, also offer prepackaged tool kits that enable data scientists to build models without coding, further democratizing access to technology innovations and data insights.""",
    """Data science is a multidisciplinary domain that combines programming, mathematics, statistics, machine learning, and artificial intelligence to analyze and unlock insights from raw data. At the intersection of computer science and business strategy, data science has become indispensable for businesses across industries.
Data science empowers organizations to decipher complex patterns, extract meaningful knowledge, and make informed decisions via advanced analytics. Data science is critical for transforming business outcomes by providing insights that drive decision-making and innovation.
So, what is the process used in data science? What are the different benefits, uses, and applications of data science? This article explores various aspects of data science and provides a comprehensive guide to understanding data science.""",
    """Data Science is a term that escapes any single complete definition, which makes it difficult to use, especially if the goal is to use it correctly. Most articles and publications use the term freely, with the assumption that it is universally understood. However, data science – its methods, goals, and applications – evolve with time and technology. Data science 25 years ago referred to gathering and cleaning datasets then applying statistical methods to that data.
In 2018, data science has grown to a field that encompasses data analysis, predictive analytics, data mining, business intelligence, machine learning, and so much more.
In fact, because no one definition fits the bill seamlessly, it is up to those who do data science to define it."""
]


def print_doc(doc):
    print(doc.replace('  ',' ').replace('\n\n','\n').replace('\n\n','\n').replace('’',"'").replace(' ',' '))